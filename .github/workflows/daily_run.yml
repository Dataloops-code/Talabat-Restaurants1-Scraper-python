name: Daily Talabat Scraper

on:
  schedule:
    - cron: "0 */6 * * *"  # Runs every 6 hours
  workflow_dispatch:     # Allows manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Update geckodriver
        run: |
          sudo rm -f /usr/bin/geckodriver
          wget https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-linux64.tar.gz
          tar -xvzf geckodriver-v0.36.0-linux64.tar.gz
          chmod +x geckodriver
          sudo mv geckodriver /usr/bin/
          geckodriver --version

      - name: Fix PhantomJS Issue
        run: |
          npm uninstall phantomjs-prebuilt || true
          npm install phantomjs-prebuilt@2.1.13 || true
          npm cache clear --force
          npm install || true

      - name: Restore cached progress
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: current_progress.json  # Match main.py’s file
          key: talabat-scraper-progress-${{ github.run_id }}
          restore-keys: |
            talabat-scraper-progress-

      - name: Run Talabat scraper
        run: python main.py

      - name: Commit progress updates
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          git add current_progress.json scraped_progress.json output/
          git commit -m "Update scraper progress and data for run ${{ github.run_id }}" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Save progress to cache
        uses: actions/cache/save@v4
        with:
          path: current_progress.json  # Match main.py’s file
          key: talabat-scraper-progress-${{ github.run_id }}

      - name: Cleanup
        if: always()
        run: |
          rm -rf node_modules
          rm -rf ~/.cache/ms-playwright

          
          
# name: Talabat Scraper Workflow
# on:
#   # schedule:
#   #   - cron: "0 */6 * * *" # Runs every 6 hours
#   workflow_dispatch: # Allows manual triggering
# concurrency:
#   group: scraper-workflow
#   cancel-in-progress: false # Don't cancel in-progress runs to avoid data loss

# jobs:
#   scrape:
#     runs-on: ubuntu-latest
#     timeout-minutes: 350 # ~5.8 hours to ensure clean shutdown before 6-hour mark
    
#     steps:
#       - name: Checkout Repository
#         uses: actions/checkout@v4
        
#       - name: Set up Python
#         uses: actions/setup-python@v5
#         with:
#           python-version: '3.10'
          
#       - name: Install Dependencies
#         run: |
#           python -m pip install --upgrade pip
#           pip install -r requirements.txt
#           pip install jq
          
#       - name: Install Playwright for Python
#         run: |
#           pip install playwright
#           python -m playwright install chromium firefox # Install both browsers
          
#       - name: Update geckodriver
#         run: |
#           # Remove the existing geckodriver
#           sudo rm -f /usr/bin/geckodriver
#           # Download and install geckodriver 0.36.0
#           wget https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-linux64.tar.gz
#           tar -xvzf geckodriver-v0.36.0-linux64.tar.gz
#           chmod +x geckodriver
#           sudo mv geckodriver /usr/bin/
#           # Verify installation
#           geckodriver --version
          
#       - name: Fix PhantomJS Issue
#         run: |
#           npm uninstall phantomjs-prebuilt
#           npm install phantomjs-prebuilt@2.1.13
#           npm cache clear --force
#           npm install
      
#       # Get progress state from previous runs
#       - name: Download Progress Data
#         uses: actions/cache/restore@v4
#         id: cache-restore
#         with:
#           path: |
#             progress.json
#             output
#           key: talabat-scraper-progress-latest
#           restore-keys: |
#             talabat-scraper-progress-
      
#       # Make sure output directory exists
#       - name: Create Output Directory
#         run: |
#           mkdir -p output
#           mkdir -p .cache
      
#       # Debug - check what was restored
#       - name: Check Progress State
#         run: |
#           echo "Cache restore status: ${{ steps.cache-restore.outputs.cache-hit }}"
#           if [ -f "progress.json" ]; then
#             echo "progress.json exists - resuming from previous state"
#             echo "Current area:" 
#             cat progress.json | grep -o '"area_name":[^,]*' || echo "No area name found"
#             echo "Current page:"
#             cat progress.json | grep -o '"current_page":[^,]*' || echo "No current page found"
#             echo "Current restaurant:"
#             cat progress.json | grep -o '"current_restaurant":[^,]*' || echo "No current restaurant found"
            
#             # Check partial files
#             find output -name "*_partial.json" | sort
#           else
#             echo "No progress.json found - starting fresh"
#             echo "{\"completed_areas\": [], \"current_area_index\": 0, \"last_updated\": null, \"all_results\": {}, \"current_progress\": {\"area_name\": null, \"current_page\": 0, \"total_pages\": 0, \"current_restaurant\": 0, \"total_restaurants\": 0, \"processed_restaurants\": [], \"completed_pages\": []}}" > progress.json
#           fi
#           echo "Files in output directory:"
#           ls -la output/ || echo "Output directory is empty"
      
#       # Run the scraper with 3-level tracking
#       - name: Run the scraper
#         env:
#           TALABAT_GCLOUD_KEY_JSON: ${{ secrets.TALABAT_GCLOUD_KEY_JSON }}
#         run: |
#           # Run with error handling
#           python main.py
      
#       # Save intermediate snapshot every 15 minutes
#       - name: Setup progress snapshots
#         run: |
#           echo "#!/bin/bash" > snapshot.sh
#           echo "while true; do" >> snapshot.sh
#           echo "  cp progress.json progress.json.bak" >> snapshot.sh 
#           echo "  find output -name '*_partial.json' -exec cp {} {}.bak \;" >> snapshot.sh
#           echo "  echo \"[$(date)] Created progress backup\"" >> snapshot.sh
#           echo "  sleep 900" >> snapshot.sh
#           echo "done" >> snapshot.sh
#           chmod +x snapshot.sh
#           ./snapshot.sh &
#           echo $! > snapshot.pid
      
#       # Save the progress for the next run
#       - name: Save Progress State
#         uses: actions/cache/save@v4
#         if: always() # Run this even if the script fails
#         with:
#           path: |
#             progress.json
#             output
#           key: talabat-scraper-progress-latest
      
#       # Upload artifacts for debugging
#       - name: Upload Progress as Artifact
#         uses: actions/upload-artifact@v4
#         if: always()
#         with:
#           name: talabat-progress-data
#           path: |
#             progress.json
#             output/*.json
#           retention-days: 7
      
#       # Kill background process
#       - name: Cleanup snapshot process
#         if: always()
#         run: |
#           if [ -f snapshot.pid ]; then
#             kill $(cat snapshot.pid) || true
#             rm snapshot.pid
#           fi
      
#       - name: Cleanup
#         if: always()
#         run: |
#           rm -rf node_modules # Clean up npm modules
#           rm -rf ~/.cache/ms-playwright # Clean up Playwright cache
